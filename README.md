# ğŸ§  ML Serving Infrastructure â€” Technical Test Submission

This project implements a complete, production-ready infrastructure and serving pipeline for deploying a Hugging Face machine learning model using FastAPI, Terraform, EKS, Prometheus, Grafana, and GitHub Actions.

---

## âœ… Functional Requirements Covered

- [x] Infrastructure as code (Terraform for AWS EKS)
- [x] CI/CD pipeline with GitHub Actions (unit test, Docker build, K8s deploy)
- [x] Monitoring dashboard with Prometheus + Grafana
- [x] API Endpoints for model management and inference
- [x] Deployment support for both **AWS EKS** and **Minikube/local**
- [x] Unit tests and modular run scripts per folder

---

## ğŸ“ Project Structure

```
ml-serving-app/
â”œâ”€â”€ iac/                    # Terraform code for provisioning EKS
â”‚   â””â”€â”€ run.sh              # One-click infra provisioning
â”œâ”€â”€ serving/                # FastAPI model server + Dockerfile
â”‚   â””â”€â”€ run.sh              # Build and optionally run locally
â”œâ”€â”€ tests/                  # Unit tests using pytest
â”‚   â””â”€â”€ run.sh              # Run tests
â”œâ”€â”€ k8s/                    # Kubernetes manifests for the app
â”‚   â””â”€â”€ run.sh              # Deploy to K8s and port-forward
â”œâ”€â”€ monitoring/             # Helm install + Grafana dashboard JSON
â”‚   â”œâ”€â”€ grafana-dashboards/
â”‚   â”‚   â””â”€â”€ ml-serving-dashboard.json
â”‚   â””â”€â”€ run.sh
â””â”€â”€ .github/workflows/ci-cd.yaml  # GitHub Actions pipeline
```

---

## âš™ï¸ Prerequisites

Ensure the following tools are installed:

- `Terraform` >= 1.4
- `AWS CLI` configured with EKS permissions
- `kubectl`
- `helm`
- `docker`
- `python3` and `pip` (for testing)
- (Optional) `minikube` if testing locally

---

## ğŸš€ How to Run Each Component

### ğŸ› ï¸ 1. Provision Infrastructure

```bash
cd iac
./run.sh
```

> This uses Terraform to deploy EKS and optionally updates kubeconfig.

---

### ğŸ“Š 2. Install Monitoring Stack

```bash
cd monitoring
./run.sh
```

> Installs Prometheus and Grafana via `kube-prometheus-stack`. Also includes a dashboard JSON in `grafana-dashboards/`.

---

### ğŸ³ 3. Build and (Optionally) Run the App Locally

```bash
cd serving
./run.sh
```

> Builds Docker image and optionally starts the container on port 8000.

---

### â˜¸ï¸ 4. Deploy App to Kubernetes

```bash
cd k8s
./run.sh
```

> Applies Kubernetes manifests and optionally port-forwards the app.

---

### ğŸ§ª 5. Run Tests

```bash
cd tests
./run.sh
```

---

## ğŸŒ API Endpoints

| Endpoint         | Method | Description                          |
|------------------|--------|--------------------------------------|
| `/model`         | GET    | Get current model ID                 |
| `/model`         | POST   | Deploy new Hugging Face model        |
| `/status`        | GET    | Get model deployment status          |
| `/completion`    | POST   | Run inference using deployed model   |

Example request:

```json
POST /model
{ "model_id": "gpt2" }
```

```json
POST /completion
{
  "messages": [
    { "role": "user", "content": "Tell me a joke" }
  ]
}
```

---

## ğŸ“Š Monitoring Dashboard

Included in: `monitoring/grafana-dashboards/ml-serving-dashboard.json`

It shows:

- Node CPU / Memory
- Requests per second
- Model load latency (p95 tracking)
- (Optional) log panel if Loki is configured

---

## ğŸš¨ Notes

- Logs are written to stdout (can be scraped by Loki if configured)
- Terraform uses the `terraform-aws-eks` and `vpc` modules
- Docker image uses `uvicorn` + `FastAPI` + `transformers` for inference
- Requests are tracked using `prometheus_client`

---

## âœ… Submission Summary

This project delivers a cloud-native ML serving platform meeting all specified requirements. It is modular, testable, observable, and reproducible.